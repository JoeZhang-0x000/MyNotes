## 2023-02-14
#### tasks
+ [x] 查看Zhiqin Xu的文章，找出与data-fit相似的内容
+ [x] 向袁老师要翼型的结果
+ [x] https://zhuanlan.zhihu.com/p/466415033 关于SA湍流模型
### diary
许志钦的文章主要和他的研究方向有关，大多提到了傅里叶变换，他认为基于梯度下降DNN会优先收敛低频部分。在MOD-Net文章中，他通过格林公式巧妙的构造了一种PINNs变体，具有一定的参考意义。比较关心的data regularization部分，许是用的粗粒度求解器获取的一部分解，这部分解也是真实的，与我们的假数据不属于一个方法，假数据所给的数据可能是完全错的，我们要抽象出的是一个中间过程，通过这样一个中间过程让我们更快的收敛，我曾经看过这样一个节目，两位参赛者，一位选择走最短的路程，但是基本上饿了两三天，靠着毅力挺到了终点，另一位选择了一个悠闲的路线，轻松的到达了终点，并且比第一位更快。我认为假数据起到的作用大抵就是如此，正所谓磨刀不误砍柴工。

## 2023-02-15
### tasks
- [x] OFM翼型计算
- [x] PINN翼型代码完成
### diary
进行了naca0012工况的PINN求解。仿照OFM的参数，设置6e6的雷诺数，水平来流(51.4815m/s)0.15Ma，但是结果发散了。后来将水平来流速度从51.4815换成了0.15，从残差收敛图上可以看出，残差下降的很快。或许需要将u,v,p适用独立的子网络去训练。
师兄说，我的zero_gradient boundary设置的有问题，我是用divp = 0，但是这个其实错的，应当用法向梯度。此外，师兄提到当量级差了10倍以上时使用batch normalization比较有用，否则可不做，并且batch normalization不能和AD一起使用，必须使用FD。最后，考虑后续涉及三个子网络来分别输出u,v,p可能性能会好一点。
另外，之前的cavity案例的压强梯度也设置错了，并且没有考虑连续性。
## 2023-02-17
### tasks
- [x] 任务书与开题报告改好提交
- [x] NACA算例调整
- [x] 论文提纲
### diary
修改任务书，与开题报告。任务书因为系统原因未提交，开题报告的会议纪要部分由于不知道相关老师的信息，未填写。
开会，袁老师说应该开始写论文了，先把论文的前面方法论部分写出来。可以先写一个提纲。
## 2023-02-18
### diary
与杰哥交流，发现是边界设置错误了。OFM之所以给出的结果是一个矩形区域而不是实际的计算域，是因为远处的那个扇形区域是无限远的流场，再往外都是均匀的，所以直接按照无限远处的信息可以填充成任意需要的流场。对于翼型问题来说，只需要关心壁面附近。
## 2023-02-19
### tasks
- [x] 论文提纲
- [x] 与袁老师交流翼型边界设定(明天晚上)
- [x] 方腔流二次训练方法
### diary
思考一下，这两种写法有何差异，
```python
loss_val_list = eq1 + eq2+ eq3 # 残差列表

    loss_val_h = loss_fn(loss_val_list,y_real[:,0])

    loss_val = loss_fn(eq1,y_real[:,[0]]) + loss_fn(eq2,y_real[:,[1]]) + loss_fn(eq3,y_real[:,[2]])

    print(loss_val,loss_val_h)
```
结果显示，两者并不相等。那么究竟是前者写法正确还是后者写法正确？
![[Pasted image 20230219185443.png]]
其实后者是错的，因为后者会存在不同维度上**相互抵消**。之前的代码有些地方写错，就是犯了第二种写法的错误。

### 边界分割的启发
在方腔流的实验中，将边界分割为多部分，每一部分独立开来训练，则效果优于合并起来，思考其原因，可能是把边界整体训练时，由于各个边界的值有正有负，可能会相互抵消。所以接下来应该实验验证，将每一块的边界残差分别打印出来，再将整体的残差打印，观测是否有影响。如果这个假设成立，那么N-S方程的扩散性，对流项也可能会相互抵消从而导致训练失败，按照这个逻辑，应当将一个复杂的方程分割为许多简单的子方程进行训练。
过了一会儿，仔细一想发现上面的说法是不准确的。应当是对于空间上的点，合并会相互抵消，拆开更好，但是对于一个方程应当是合并。对于方程中的各个项，可能发生抵消的原因是梯度相互的抵消。
所以要验证这两种抵消，我们需要设计相应的实验，第一种空间上的分割，主要是针对于边界条件，将边界合并到一个batch中，和将边界分割成多个batch，最终的结果进行对比。
#### 实验设计
合并bc，将bc按照速度和压强设置，
```python
bc_uv_ipt = np.vstack([bc_up_ipt,bc_left_ipt,bc_right_ipt,bc_down_ipt]) # 速度边界

bc_dpdn_zero_ipt = np.vstack([bc_left_ipt,bc_right_ipt,bc_up_ipt,bc_down_ipt]) # 压力梯度为零

bc_uv_opt = np.zeros([bc_uv_ipt.shape[0],2])

bc_uv_opt[:bc_up_ipt.shape[0]] = [U_ABS,0]

bc_dpdn_zero_opt = np.zeros([bc_dpdn_zero_ipt.shape[0],2]) # 代表相应的法向量

bc_dpdn_zero_opt[:bc_left_ipt.shape[0]+bc_right_ipt.shape[0]] = [1,0] # 左右两边的壁面，法向量为(1,0)

bc_dpdn_zero_opt[bc_left_ipt.shape[0]+bc_right_ipt.shape[0]:] = [0,1] # 上下两册的壁面，法向量为(0,1)
```
分项bc1，按照边界的类型分割bc
```python
bc_uv_zero_ipt = np.vstack([bc_left_ipt,bc_right_ipt,bc_down_ipt]) # 左，下，右三个壁面无滑移

bc_uv_up_ipt = bc_up_ipt # 上界面的速度条件

bc_dpdx_zero_ipt = np.vstack([bc_left_ipt,bc_right_ipt])

bc_dpdy_zero_ipt = np.vstack([bc_up_ipt,bc_down_ipt])

  

## 构建output

pde_opt = np.zeros([pde_batch_size,3]) # u v p

bc_uv_zero_opt = np.zeros([bc_uv_zero_ipt.shape[0],2]) # u=0,v=0

bc_uv_up_opt = np.c_[np.ones([bc_uv_up_ipt.shape[0],1])*U_ABS,np.zeros([bc_uv_up_ipt.shape[0],1])] # u = U_ABS, v = 0

bc_dpdx_zero_opt = np.zeros([bc_dpdx_zero_ipt.shape[0],1]) # dpdx = 0

bc_dpdy_zero_opt = np.zeros([bc_dpdy_zero_ipt.shape[0],1]) # dpdy = 0
```
分项bc2，按照边界的类型以及边界的几何信息，分割成每一条直线都有一个速度边界一个压强边界，
分项bc3,在分项bc2的基础上，按照维度再划分，每一个维度一个batch训练


#### 结果
##### 5k步
左为分项边界bc2，右为整合边界bc1
![[Pasted image 20230220121746.png]]

##### 10k步
左为分项边界bc2，右为整合边界bc1
![[Pasted image 20230220005337.png]]
#### 分析
所以我们发现其实分项的效果是优于整合的。开始时候我认为是因为残差相互抵消影响。但是随后仔细一想残差都是平方后的，因此不存在抵消。那么只能是梯度相互抵消了。所以我们还要做一个实验分析梯度的影响。
### 关于权重的一种想法
最初的时候按照师兄的代码，权重应当都是1，后面袁老师的想法是各项的权重之和为1。于是我把代码改成了，1/权重数量，但是这样是不恰当的，应当改为这一个batch中这种训练点的数量占总量的百分比。所以我把代码里面的权重更正了。
```python
ls_weight_list = np.array(batchsize) / sum(batchsize)
```
另外就是权重和一定要等于1吗？权重体现在神经网络中，主要作用是作为一个斜率增强。残差的数值其实不重要，重要的是残差的斜率，大的一个权重和将会带来大的一个斜率，但是这样和我增加学习率有什么区别吗？好像没啥本质区别，一个是在小尺度上的缩放，一个是大尺度上的放大。但是我认为大尺度更好一点，因为计算机的物理结构导致了其天生适合存放大尺度数据而非小尺度数据。

## 2023-02-20
### task
- [x] 完成train-2nd的实验对比
- [x] 制作今天会议的ppt，包含train-2nd实验分析，拆分权重实验分析，拆分权重梯度的实验分析，以及aerofoil的实验
- [x] 整理要问的问题
### diary
#### train-2nd实验设计
train-2nd是我给二次训练起的一个临时的名字，正如邱师兄之前说的那样，解决长尾问题重采样是一种较好的方法。train-2nd正如同重采样有着异曲同工之妙，但是可能和其他领域不太一样，对于pinn来说，我们的目的是获取物理场中任意一点的偏微分方程的解，因此其实在一个点附近采多个点和我把这个点训练多次都能提高这一个点邻域的精度。(这里**我突然想到一个idea**,就是训练的时候给所有的样本点一定的小扰动，以模拟其delta领域内的结果，首先我们有理由认为，物理方程在某一个比较小的区域内的性质是相似的，因此如果给的扰动较小，就可以模拟在这个领域内所有点的性质，从而我们的采样点相当于变多了，但是训练的成本却没有增加。)
言归正传，train-2nd的实验设计方案如下：
1. 我们的**目的**是验证train-2nd的优越性，train-2nd将一定百分比通常为20%的训练数据重新训练一遍，我们这样做的理由是这20%是尾部数据，其训练结果不理想，本来我们应该在这些数据附近重采样，但是现在多给他们训练一轮，因为其他数据是训练好了的，所以我们有理由只关注尾部数据。
2. 实验应当以是对照试验。设置一个对照组是不使用train-2nd的，另一个是使用train-2nd的。但是考虑到一个train-2nd每一轮训练的样本数其实比空白组要多20%，所以训练的轮数应当减少。
3. 如果train-2nd对于精度提高有效果，那么也就是说更少的轮次可以获得更高的精度，也就是有加速效果，我们应该尝试测试出加速效果的定量形式。现在设计实验去减少train-2nd的训练轮次。之前是保持训练总量不变用了缩减20%的训练轮数，现在我们减少30%,40%,50%...直到两者精度相近。更改原来的代码，改为可以在训练过程中记录误差。先把真实数据导入，再设计一个函数，执行之后用神经网络给一个结果，然后记录三者的误差，最后统一保存。

### 实验结果
#### 分项bc2
##### 0.8
origin 5k,train-2nd 4k，左为train-2nd，右边为origin，精度有明显提高
![[Pasted image 20230220140656.png]]
origin 10k,train-2nd 8k，左为train-2nd，右边为origin，精度提高效果减弱
![[Pasted image 20230220140906.png]]
##### 0.7
origin 5k, train-2nd 3.5k
origin 10k, train-2nd 7k
##### 0.6
origin 5k, train-2nd 3k
origin 10k,train-2nd 6k
##### 0.5
origin 5k,train-2nd 2.5k
origin 10k,train-2nd 5k
##### 0.4
origin 5k,train-2nd 2k
origin 10k,train-2nd 4k
#### 合并bc
##### 0.8
origin 5k,train-2nd 4k，左为train-2nd，右边为origin,精度有明显提高
![[Pasted image 20230220141441.png]]

### 踩坑
调试时候踩了一个大坑，开始不理解为啥师兄用的全是诸如``y[:,[0]]``这样的方法去索引，为啥不直接用``y[:,0]``，好吧原来前者的输出是一个[?,1]维的向量，而后者是[?,]维的向量，这样会引起计算时候的出错，加减可能没有影响，但是乘法运算时候很容易出错。所以要时刻谨记向量的形状保持一致。

### datt-fit权重
为了让权重和为1，更改了一种data-fit权重方法，那就是给data-fit部分增加额外的幽灵点，使得data-fit部分占整体的百分比提高，但是幽灵点的数量会逐渐降低。
```python
        t_bs = copy.deepcopy(batchsize)

        ghost_ratio = 2/np.exp(step/100)

        t_bs[-1] *= ghost_ratio

        ls_weight_list = t_bs/np.sum(t_bs)
```

### 无量纲化训练
考虑将ns方程无量纲化，否则速度压强的量级差别太大，不利于训练。

### 更精细的数据
使用了师兄的数据集之，发现二次训练精度降低了，但是师兄的数据集是没有二次涡的，会不会是因为网格不够密，然后OFM获得的结果误差较大。考虑加密网格之后，获取更高精度数据再来对比。

### naca训练时候的一个问题
如图所示，可能是初始权重的关系。也可能是这个断面针对满足ns方程。

### 不能排除代码写错
最初的时候训练结果蛮好的，只是需要，可是下午时候为了

### 会议纪要
1. 针对壁面附近加强采样
2. 证明二次采样的效果可以对比lulu的自适应采样
3. resampling在前期不使用，因为其训练本来就不好，在后期认为大部分的区域都已经收敛到一个较好的结果再用
4. 先用精确解来看，问题复杂程度和网络规模的关系，直接做数值驱动，先试验出网络的结构
5. OFM速度改小一点，这样量级就不会影响
6. 使用核函数映射距离，或者用对数
7. 做data-fit时候只对机翼外侧的区域，机翼周围不做

## 2023-02-22
### tasks
**顺推到下一天**
- [x] OFM更密网格计算，我们需要精确解
- [x] 方腔流二次训练的结果

### diary
出现了梯度相互抵消的真实案例，很直观，完美的对称。
![[Pasted image 20230221004231.png]]

今天头疼一天，没有进展。
## 2023-02-22
### tasks
- [x] OFM更密网格计算，我们需要精确解
- [ ] 方腔流二次训练的结果(顺延24号)

## 2023-02-23
### diary
openfoam是按照时间步来计算的，所以要有足够长的实践步保证收敛，之前的结果是没有收敛的。pinn对于非定常可以直接获得稳态解，不需要时间步推进。二阶涡的解已经获得。明天可以重新开始跑算例。

## 2023-02-24
### tasks
- [x] 修改会议纪要
- [x] 制作汇报ppt

## 2023-02-26
### diary
跑雷诺数1k的案例，纯数值驱动的方法跑了25k步收敛，物理驱动4w步并没有收敛。师兄说，高雷诺数不好收敛，但是我想要跑出来一个好看的结果。我觉得主要是边界的值收敛的不好，要不然试试硬约束。
翻译了一篇文献的摘要和introduction,1k多字，估计翻译个5篇就差不多了。

## 2023-02-27
### diary
用OFM层流计算了速度为1的，粘性系数为1e-5的流动。更改PINN代码，采样方式改为在大范围内随机采样和在壁面附近围绕着壁面采样，但是还是没有训练出能用的网络。水平速度收敛的还可以，竖直方向速度完全不能看。考虑再咨询袁老师这个问题。

## 2023-02-28023-02-28
### diary
### 袁老师交流
早上袁老师打电话，说了两个点。
1. 用迁移学习，先学习欧拉，再学习ns
2. 分析一下梯度相互抵消

### 精确解做数据正则项
下午时候用了精确解去做data-fit，效果还好，能够收敛。并且比纯的数值驱动要快。所以跑翼型这个案例我们可以采用少量的粗网格数据。纯的数值驱动，使用openfoam的网格点的数据拟合，大概25k步收敛，使用pinn+data，仅仅1w步就收敛了。

### 子网络
修改了一下网络的结构，先用FC[2,64,64,64,64,3]，然后再对每个输出用子网络FC[1,16,16,1]进行映射，试图以此来降低u,v,p三者之间的数量级的影响。结果提升不大。但是对于量级有很好的控制效果，至少u,v,p的量级正确了，后面继续使用再看看。

### 动态权重
根据残差来做动态权重，权重为 exp(Li) / sum(exp(Li))，i ∈{pde,inital,boundary,data}。动态权重不应该是一直执行的，应该每隔几个训练步执行一次。
疑问，这里用softmax好还是不用比较好。
用了softmax是不是相当于在一定时间内只由一项Loss主导训练。

### 采样
发现机翼竖向方向结果不好，考虑研着机翼的两侧夹成的区域加密采样。
![[Pasted image 20230228234557.png]]
### 关于知乎
知乎文章，pinn是一个坑吗那篇文章写的还不错，里面有一些有参考意义的例子，比如用pinn去做轨道优化，传染病预测，这些都很符合时事论文里面可以提一提。

## 2023-03-01
### diary
#### 关于变分
学习有限元的基础知识，这次总算是理清楚了galerkin法。我有两个想法，
1. 补充结构力学的例子，求势能的最小值，从而求解结构力学变形问题，这个相对来说有解析解更好参照，可作为论文一小章节。
2. ns方程只是描述了动量守恒，增加最小化势能原理，让整体的势能最小化。这样对于流动问题是否可以获得更精确的解。或者有利于翼型的收敛。
#### 关于有限元
我所理解的有限元仅仅是离散化。有限元是一种辅助，就像样条曲线一样，它的本质是将一个原本用ritz-galerkin法难以收敛的问题划分成多个子区域，每个子区域再分别使用ritz-galerkin求解。偏微分方程只在特定的边界条件下有解，因此边界的引入十分重要。ritz-galerkin法都假设原函数可以被表征为一组正交基函数的线性组合，而这些基函数需要满足边界值条件，因此组合出来的原函数也一定会满足边界条件。这些基函数一般是事先构造出来的，也就是已知的，所以我们需要求得相应的系数。这里就产生了两种分歧。ritz法出现的比galerkin要早很多，ritz法是直接变分法的一种，通俗的说ritz法具有相应的物理意义，即让系统的势能处于最小状态。由于高等数学可知，势能函数取得极值的时候，一定处于极值点，极值点的导数为0，因此对势能函数直接求微分，使其为0，最后转化成为解一个线性方程，就可以求得基函数的系数，从而得到了原函数的近似表达。而galerkin则应用的更为广泛一些，galerkin是加权余量法的代表。对于一个偏微分方程及其边界条件，我们对等式的两边同时乘上任意一个连续函数，再两边同时做积分，则等式仍然应该成立，这个被施加去与原方程求内积的函数被称为试函数。但是由于我们采用了基函数线性表征的解是一个近似解，近似解不一定在定义域以及边界上处处严格满足偏微分方程的条件，因此其与试函数的内积不为0。我们取多个试函数，将他们的与近似解所构造的偏微分方程产生的内积做一个加权和，产生的余量或者叫残差就是衡量我们这个近似解精度的标准。因此，我们需要做的就是优化这个加权余量，使得其取到极小值。常见的两种试函数取法，第一种是最小二乘法，即将余量自身作为试函数，第二种是galerkin法，将基函数作为试函数。并且经过证明，当变分原理存在时，galerkin加权积分和变分原理是一致的。

#### 关于naca0012
昨天对机翼所在的十字形矩形区域进行了加密采样，今天跑了几次，发现速度v的精度有了一些改善，但是精度还是不太好，所以考虑进一步把水平速度和竖直速度分开。此外，采样还是不够密。主要体现在竖向方向，出现了间断点。所以考虑在竖向再加密。此外，训练时候经常会发现pde_loss大于壁面的速度余量，估计是softmax起作用了，让速度收敛的比较快。考虑后续实验也要加密pde配置点。
经过我大量的实验证明，uv合并难以收敛，因此现在尝试将水平和垂直速度分开。

#### 关于softmax
一些论文中关于动态权重使用的是softmax，经过实验发现softmax比直接的求百分比要好，如果直接每个loss都用相应的元素除以加和，那么会造成残差振荡。因此应当使用softmax，但是为什么softmax会有效的避免我还是不清楚。![[Pasted image 20230301200104.png]]

#### 我的galerkin猜想
看了那么多有限元，我也尝试将神经网络与有限元结合。我的中心思想只有一条，那就是充分利用自动微分的便利性以及神经网络的普遍近似性。因此有以下几种思路，
1. 用随机映射充当试函数，因为神经网络可以近似出任何函数，所以我们给他一组随机的权重，就可以得到一组试函数，虽然我不知道怎么构造一个比较好的试函数，但是我们的量大管饱。
2. 用神经网络学习边界，从而得到基函数，然后类似于参数反演，我们就可以通过构造的近似解利用自动微分获得导数，组装弱形式，利用伽辽金法直接求加权余量，最后最小化损失。
## 2023-03-02
### diary
#### 关于galerkin
很难实现。神经网络偏向于收敛到全为0这种极端值，尽管我增加了高阶导数不能为0的惩罚项，效果甚微。以后再说吧。暂时告一段落。

#### 迁移学习
明天应当首要任务是把代码整理出来，弄成一个类一个算例，然后构造不同的类通过给同的参数，这样可以在超算上跑多个模型。
对于naca的算例可以试试迁移学习，先学欧拉，再冻结几层学ns。那么这样可不可以先学边界，再同时学边界和pde，优点像袁老师说的边界延后导入，但是与那个相反。因为感觉边界难以学习，所以我认为要在满足边界的基础上找让pde余量最小的。

## 2023-03-03
### tasks
- [x] 文献翻译

## 2023-03-07
### diary
写力学这边要求的文献综述，洋洋洒洒写了5k字，交给老师看，科普性，学术性还是太稚嫩。题目太大了，应当主讲pinn，而不是花大量笔墨对流体力学和计算流体力学进行叙述。
接下来写软件这边要求的文献综述，主要就对PINN进行分析。

## 2023-03-10
### diary
用超算跑出来的结果，感觉也不是太好。但是或许我们可以不只根据云图来判断，尝试一下通过气动力来判断？
接下来查阅一下相关的文献，重点看看那篇application of transfer learning...的文章，看看他怎么设置评价指标的。

## 2023-03-11
### diary
之前的网络模型用的不合理，前面一个2,64,64,64,64,3的网络，然后前面网络的输出再接一个1,16,16,16,1的子网络。但是我看了下别人论文，发现应该是子网络和前面那个网络的输出层全连接而不是部分链接，所以改成了[2,64,64,64,64] x [64,16,16,1]x3

## 2023-03-12
### diary
网格采的很密也没用，对于机翼表面的速度场还是很难模拟。或许需要一定的粗网格数据。自适应权重的方法感觉还是不能放大偏大的残差，要试试根据梯度算的那个，但是如何实现。
一旦带入了粗网格数据，那么顺理成章的是去用不同攻角、马赫数的数据做data-fit，但是这样的话为何不直接做迁移学习？或许还可以直接前面只训练数据部分，后面迁移时候再加入pde约束？
以及昨晚重新看了一下那篇application of TL ...的文章，文中作者用了一部分粗网格数据做data-fit，他所谓的粗网格是粗网格的欧拉，然后迁移学习到其他攻角的RANS+SA。如果论创新点，似乎我们做的就是人家做过的。但是值得注意的是，文中使用的ns方程的形式。

### 向量形式
经过调研，文中所用的euler equation,ns equation均为向量形式。而之所以显含ρ是因为研究的是可压流，当马赫数<0.3时，由速度引起的密度变化不到5%，因此通常认为0.3马赫以下的是不可压流。

## 2023-03-21
### diary
进行了网络结构的优选。认为对于方腔来说，母网络在2,32,32,32,子网络在32,32,1效果比较好。
![[Pasted image 20230321225800.png]]
下一步看看可以试试可视化网络的优化方向。
将这两点作为创新，创新点在于提供了一种通用研究的思路。

## 2023-03-22
### tasks
- [ ] complete the introduction of essay
- [ ] show grad visually

### diary
看了一点文献，关于VAE,WGAN,贝叶斯网络的。今天实验结果不太好，就没有完成两个可视化的工作。
把损失函数改用了L2范数，误差部分用了L2范数，这样就不会出现全0的影响了。
今天的实验感觉是学习率偏大了。产生震荡。调小学习率看看。

## 2023-03-23
### task
- [x] 论文引言国内外研究部分
- [x] 看一下BN的实现

