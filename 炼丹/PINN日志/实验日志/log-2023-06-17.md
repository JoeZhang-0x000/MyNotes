## 同伦算法
### 同伦牛顿迭代
同伦算法的核心应当是处理一个复杂问题时, 逐步从简单到复杂的处理相似问题。实际可以看作一种初始化手段。
例如在牛顿迭代时，若解的导数接近0，则更新时的步长容易爆炸，进而导致难以收敛，甚至发散，出现这种不稳定性的原因是初始值的选择不当。合理的选择初始解可有效的降低牛顿所需的迭代步数以及提高收敛精度。
同伦牛顿迭代实际就算将原问题
$$
f(x) = 0
$$
转化为
$$
\begin{aligned}
& h(x) = 0 \\
& tf(x) + (1-t)g(x) = h(x)
\end{aligned}
$$
$g(x)$是一个辅助函数，相对于原问题$f(x)$容易求解，通过逐渐的将$t$从$(0,1)$可逐渐将$h(x)$变为$f(x)$，实际求解中我们使用的是离散化的$t$,于是可以视作$t_{n}$时刻的初始值为$t_{n-1}$时刻的解，于是避免了初始化不当引发的发散和收敛问题。
### 同伦PINN
文献[[Yao Huang et al.(2022)]]中提到过同伦PINN，但是该文中使用同伦算法解决不适定方程中的多解问题。我认为这并没有充分发话同伦算法的魅力，处理多值问题可能更关键的是找出一个函数族或者泛函而不是找出几个采样点，该文献中给$t$离散的值来构造出了许多不同的解，但是这些解都可看作是从某个函数空间中的一些采样。
而我们所谓的同伦PINN是要解决初始化的问题，对于复杂问题，我不去直接训练，而是由简而繁，用简单问题的解作为复杂问题的初始化。这只是一种思想，或许只是套着同伦延拓的壳子也不太精确。因此，同伦PINN可以是由很多部分组成。
### 参数初始化的同伦
最为狭义上的同伦PINN。
例如我们要优化的损失函数是
$$
\mathcal{L_{all}}=\mathcal{L_{f}}+\mathcal{L_{b}}
$$
其中$\mathcal{L_f}$代表控制方程的损失函数，$\mathcal{L_b}$代表边界条件的损失函数。我们可以将其变形为
$$
\mathcal{L_{all}} = t(\mathcal{L_f}+\mathcal{L_b})+(1-t)\mathcal{L_h}
$$
and let $t$ from 0 to 1.
$\mathcal L_h$ represents an auxiliary loss function which can take any kind of loss function, such as the data loss, the boundary loss or other PDE loss.
for example, auxiliary function mentioned here can be the *false data loss function* we proposed earlier.

### 采样空间的延拓
Continuation of sampling space refers to gradually increasing sampling size. 
We have a prior hypothesis that most of the early parameters of a neural network are mostly chaotic, leading to significant residual for the majority of sampling points. Therefore training a neural network with large points at first would be inefficient. As the parameters of the neural networks gradually stabilize, it starts to possess some initial recognition capability for the problem. At this point, we could fine-tune the NN by adding sampling points, because majority of them exhibit smaller residuals.
### 复杂问题的迁移
Transfer Learning of complex problems involves simplifying or transforming certain challenging of the problem. This is achieved by gradually restoring them to their original form through a continuous process. By employing this approach, it becomes possible to efficiently train complex problems. 
for instance, in the training of N-S equations, the diffusion term, involving second-order derivates, is typically more difficult to train compared to the convection term. To address it, a strategy can be employed where the weights associated with the diffusion term are initially set to zero during the early stages of training. This enables the NN focus on learning convection term at first. As the training progresses, the weights of the diffusion term are introduced and adjusted, allowing the NN to progressively learn and incorporate its effects. This training strategy allows the NN to capture and handle both the convection and diffusion terms of the N-S equations. 
for example, the N-S equations can be written as:
$$
\rho \left(\frac{\partial v}{\partial t}+(v\cdot\nabla v)\right)=-\nabla p+\mu\nabla^2v+f
$$
To simplify it, we will only focus on the two-dimensional incompressible scenario.
$$
v\cdot\nabla v = -\nabla p + \mu \nabla^{2}v 
$$
and the *loss function* can be:
$$
\mathcal{L}=v\cdot \nabla v + \nabla p - \mu \nabla^{2}v
$$
it is hard to train N-S equation directly especially when $\mu$ are very small.
thus, we could train the following *loss function*  instead of the one mentioned above:
$$
\begin{aligned}
& \mathcal L = v\cdot \nabla v + \nabla p - t\mu \nabla^{2}v \\
& 0 \le t \le 1
\end{aligned}
$$
