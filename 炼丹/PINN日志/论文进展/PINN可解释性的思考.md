## 源头
在上算法课时，老师讲述他训练NN解决算法问题的案例。其中有一个排课问题，该问题是一个贪心问题。传统做法中，人类的选择是，每次选取最早下课的课。于是，老师构造了数据集，其中每个数据集的问题规模为5，即五节课中的最优排课，每节课的信息包括上课时间和下课时间，然后利用传统贪心算法求得最优选课，为一个one hot向量，例如$[1,0,1,0,1]$表示选取第1，3，5节课。将训练集交给一个四层的全连接神经网络训练，输入层有10个神经元，隐藏层两层，每层也是10个神经元，输出层是one hot编码的5个神经元。于是在该问题上，NN得到了98%的正确率。于是算法老师将隐藏层中的某一个神经元“遮掩掉”，以观察解的形式。最终发现遮掩不同的神经元输出的解并不相同，有些神经元倾向于选择下课最早的课程，有些神经元倾向于选择上课最晚的课程，前者与人类的算法规则相同，后者却是一条全新的规则，由NN自己发现。
## 思想
神经网络经常被当作一个黑盒子，我们对其内部细节知之甚少。这种“遮掩掉”部分神经元，观察其输出变化的研究方法，对于神经网络的可解释性十分有帮助。PINN有别于传统回归和分类问题，其本身就具有复杂的数学特征，倘若找到一种度量方式，评价“遮掩掉”某些神经元后解的影响，是否就能从一定程度上解释神经网络。所幸，这种度量方式也许已经被我找到，即MIC。
## 实验设计
如果这种解释性可靠，那么应当并不是仅仅针对于PINN，而是针对于全连接网络。所以不妨以简单的函数近似问题作为引子，以观察NN的神经元学习到了什么。
所以我做了一个规划，大概是四步走，从简单到复杂的顺序，逐步去验证分析。
### 简单函数近似
### 复杂函数近似
### ODE求解
### 复杂PDE求解