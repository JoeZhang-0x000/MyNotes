## 标注说明
本来作为一篇调参的记录，后来不知不觉写了许多，觉着可以整理为一篇结构与语言不太严谨的小短文了。
本文可能会使用的标注有：
💡，灯泡标志，代表一些我强烈认可的创新想法或可行解决方案，我准备积极采纳和实施；
🤔，思考标志，代表一些想法，通常对其可行性持有一定的保留态度，存在一些模棱两可的因素，需要进一步思考和评估；
😰，流汗标志，标识一些与预期不符、不够严谨或容易受到批评的部分，需要进一步完善和改进。
## 术语说明
### 凹凸
凸凹这一块一直容易混淆，国内外许多教材没有很好的区分concave和convex，有些用凹  (concave)，有些用凸 (convex)。为了以免我的理解错误导致的描述不当，在这里先规定好凹凸的性质。
下文所有讨论的都是“凹”的情况，具体函数形状可以参考下面两张图：
![[Pasted image 20231025201059.png]]
![[Pasted image 20231025201147.png]]
统一简写为 concave up 和 concave down。

### 方程的主要成分与次要成分
本文研究的对象Klein方程如下所示。
$$
u_{tt}-u_{xx}+u^{2}=-x\cos(t)+x^{2}\cos^{2}(t)
$$

| 成分                | MIC  |
| ------------------- | ---- |
| $u_{tt}$            | 1.00 |
| $u_{xx}$            | 0.27 |
| $u^2$               | 1.00 |
| $x\cos(t)$          | 1.00 |
| $x^{2}\cos ^{2}(t)$ | 1.00 | 
我们将MIC值大的项成为主要成分，MIC值小的项($u_{xx}$)称之为次要成分。这里的判断方法很主观。但是本方程恰巧其他只有一项的MIC极小，其他几项的MIC值都接近1，所以判断$u_{xx}$是次要项是合理的。
### 渐进因子
这个名字是临时用于理解用的，描述为==逐渐调整方程中次要项比重的系数==，通常由一个==光滑连续单调函数诱导==，例如渐进因子为0，就是不训练次要项，渐进因子为1，就是正常的训练模式。
渐进因子的值域为: [0,1],
诱导函数可以用三种曲线来描述，分别为linear, concave up, concave down,
渐进因子从0到1，或者说收敛于1，所花费的训练步数，占总花费的比例，称为==渐进比例==，是一个常数，取值范围是[0,1]，取0时就是标准模型，取1时就是完全渐进模型。
例如，采用一个concave up曲线来指导渐进因子的变化，设渐进比例为0.3，总共训练20k步，则渐进因子应该在$0.3 \times 20k = 6k$步时候收敛为1，其形状如下图所示：
![[Pasted image 20231025203008.png]]
### 渐进比例
即渐进训练所占总训练步数的比例，详见上一条目。
### 渐进终止点
渐进因子通常由一个光滑连续的单调函数决定，渐进过程就是函数值从初始值0接近1的过程，那么渐进终止的点就是曲线的稳定点，梯度为0的点。
### 渐进平滑系数
这个系数用于描述渐进训练过程中，曲线的平滑程度。
因为我们渐进因子其实是离散的。所以，这里的渐进平滑系数就是描述这个渐进因子变化的过程有多么平滑。
这里采用一个正整数来描述，例如渐进平滑因子为n，则将渐进的过程分为了n分。例如：
![[Pasted image 20231025204334.png|渐进平滑因子=10，即渐进因子从0-6k等分为了10段，每一段一跳。]]
### 最佳误差
训练得到的误差序列中，最小的误差。
### 收敛
这里的收敛是定性判断的收敛，因为L2误差的是一直在波动的，所以很难精确的找到收敛的稳定点。最开始想到，定量的取出每一个序列的最优点delta领域内最左端的点，认为这个点以及很接近收敛了，用这个点来判断收敛位置。
🤔这里是否需要定量的给出一个表格？给表格的目的是看起来更严谨，但是判决阈值的确定又太具备主观性了，这似乎与定量分析的初衷相违背。
例如，选取判决条件为L2(epoch=E_1) - L2(epoch=E*) < delta，并且delta=1e-3,则会出现：
![[Pasted image 20231025230244.png]]
这种就明显看起来很难看，从主观上来看，我们完全可以认为在7500步左右已经收敛了，后来的下降已经微不足道了。反正判决阈值的选取是主观的，还不如直接直接主观的看，所以后文关于收敛速度都只是一个定性的判断。
## 固定的参数
采样目前固定为下图所示，因为这个采样下训练20k步，只需要花费4个cpu时，同时4个核跑，就只要一个小时了，再密就要等太久了，不方便调参。
![[Pasted image 20231024122329.png|$x\times t=50 \times 100$，t分成100段，x分为50段。]]
初始学习率lr_0 = 1e-3
渐进平滑因子 = 10
神经网络: [2,20,20,20,1]
随机数种子: 42
## 渐进算法(only)
### 曲线类型的影响
#### 渐进比例 0.2
![[Pasted image 20231025233735.png]]
![[Pasted image 20231025212450.png|渐进比例固定为0.2，三种渐进模式均收敛快于标准模型，且最佳误差低于标准模型。其中，linear与concave up模型收敛速度最快，concave down曲线最佳误差最小。]]
#### 渐进比例 0.4
![[Pasted image 20231025233804.png]]
![[Pasted image 20231025212523.png|渐进比例固定为0.4，均表现为收敛速度优于标准模型，且最佳误差小于标准模型。其中，linear模型和concave down模型收敛最快，concave up模型最佳误差最小。]]
#### 渐进比例 0.6
![[Pasted image 20231025233843.png]]
![[Pasted image 20231025212551.png|渐进比例固定为0.6，均表现为收敛速度优于标准模型，且最佳误差小于标准模型。其中，linear模型和concave down模型收敛最快，concave up模型最佳误差最小。]]
#### 渐进比例 0.8
![[Pasted image 20231025233912.png]]
![[Pasted image 20231025212609.png|渐进比例固定为0.8，均表现为收敛速度优于标准模型，且最佳误差小于标准模型。其中，concave down模型收敛最快，linear模型和concave up模型最佳误差最小。]]
#### 渐进比例 1.0
![[Pasted image 20231025233931.png]]
![[Pasted image 20231025212623.png|渐进比例固定为1.0，均表现为收敛速度优于标准模型，且最佳误差小于标准模型。其中，concave down模型收敛最快，linear模型和concave up模型最佳误差最小。]]

### 渐进比例的影响
#### 线性渐进模型 linear
![[Pasted image 20231025233149.png]]
![[Pasted image 20231025200059.png|标准模型与线性模型 (不同渐进比例) L2误差对比。结果显示，==渐进比例越大最佳误差越小==，但是误差的减小逐渐减缓，最佳提升为从标准模型 (可视为渐进比例0) ->渐进比例为0.2的模型，即从第一幅到第二幅。收敛速度，有明显的规律是==渐进比例越大，稳定点越晚，收敛的越慢==。]]
#### 上凹曲线渐进模型 concave up
![[Pasted image 20231025233238.png]]
![[Pasted image 20231025212046.png|标准模型与concave up模型 (不同渐进比例) L2误差对比。结果显示，所有采用渐进模型的最佳误差都小于标准模型。同时，收敛速度与渐进比例呈负相关，==渐进比例越大，收敛的越慢==。]]
#### 下凹曲线渐进模型 concave down
![[Pasted image 20231025233255.png]]
![[Pasted image 20231025211951.png|标准模型与concave down模型L2误差对比。结果显示，采用concave down训练的模型最佳误差都低于标准模型，但是与渐进比例之间的关系并不明确。同时，收敛速度上几乎没什么区别，与渐进比例有一点略微的正相关，比例越大，收敛的越快。]]
### 总结
这是测出来的所有情况，着这图包含了所有信息，但是比较难看，所以拆分了多张图来看。
![[Pasted image 20231025212328.png]]
### 观察到的结论
测完的结果感觉就是没有一个普遍的规律。但是如果剔除少数例外的话，还是具备一些结论的。
#### 在收敛速度上
- 渐进比例与收敛速度呈负相关 (😰此处样本过少，还需要增加其他方程的实验，进行统计检验才可以得出结论)，concave down曲线除外；
- concave down和concave up各有千秋，从统计到到结果来看，在渐进比例小的时候，concave up更快，在渐进比例较大的时候，concave down更快，而linear则大部分处于两者之间；
#### 在最佳误差上
- 渐进比例对不同曲线的影响不同。对于linear模型，渐进比例越大，最佳误差越小，呈负相关，对于concave down模型，则呈正相关，而concave up模型则先是负相关再是正相关；
- 在固定渐进比例的情况下，最佳误差大部分情况下满足，concave up\<linear\<concave down (只有一个例外，即渐进比例为0.2时)；
- 渐进比例的增加对误差的影响并不是线性的，上述实验的结果中，从0->0.2的提升是最明显的；
#### 关于使用该算法的建议
- 首先需要使用 coarse-grained 解来判断方程中是否存在次要项，然后考虑给次要项添加一个渐进因子；
- 渐进比例的选择建议为0.2，因为从0->0.2的提升是最明显的；
- 诱导函数建议选择为linear，💡因为其简单容易实现，需要调试的参数较少，并且综合性能尚可；
### 讨论
#### 不讨论损失函数
一直以来我是回避直接讨论损失函数的，并且想在文章中用L2误差代替损失函数。原因有三：
1. 😰损失函数一直在震荡，并不好看 (读者看到的第一眼一定是觉得我学习率设置的大了，然而学习率我不方便调一个奇怪的数字，所以应当取一个折中——退火学习率，但是没有调出比较好的参数)，并且容易让读者产生歧义；
2. 损失函数中展现出来的问题，总损失函数中占主要影响的是 Dirichlet 边界和 PDE ，但是我们的方法只考虑的PDE做变型。直观上，人们会觉得只解决了一半的问题。(🤔所以我想是否应该给边界也渐进一下，但是边界一般都比较简单，渐进与不渐进真的有区别吗?)
3. 我一直持有怀疑的态度，PINNs的损失函数单独的任何一项$\mathcal{L}_{r}$，$\mathcal{L}_{b}$，写成最小二乘的形式可以从变分法理解，但是总的损失函数是$\mathcal{L}_{r}+\mathcal{L}_{b}$并不能很好的度量出当前神经网络预测的解与真实解之间的误差，例如$\mathcal{L}_{r} \to 0$与$\mathcal{L}_{b}\to 0$都会导致总的损失减小，但是哪一种减少是更主要的呢？用L2误差，直接从解的真实值出发，但是代价就是无法知道某一项的影响大小。
💡总的来说，损失函数能展示出很多信息，但是其中有用的信息可以使用L2误差来展示，剩下的无用的信息，对于我的方法是有不利影响的，就不需要展示了。
![[Pasted image 20231026192537.png|标准模型的损失函数，从损失函数中看到，影响着模型的误差的最主要因素是橙色和绿色曲线 (分别代表左侧和右侧的 Dirichlet 边界收敛情况) 与蓝色曲线 (PDE 的收敛情况)。虽然三者都在抖动，但是，从振幅以及均值上来看，橙色曲线 (左 Dirichlet 边界)最大。在总的损失函数中，三者贡献可能比较接近，但是我的方法只考虑了PDE的残差问题，并没有考虑到其他两项。也即是说，直观上给人感觉只解决了一半的问题？ ]]
#### 为什么先忽略次要成分，而不是先放大次要成分？
本文中的其他所有实验的核心思想都是，先忽略方程中次要成分的影响，那么为什么要忽略次要成分的影响？
1. 抓住主要，忽略次要，符合人的正常思维；
2. 通过实验，忽略主要，抓次要，结果并不好。
![[Pasted image 20231026161111.png|将MIC值大的成分进行渐进训练的结果。结果显示，虽然在一定程度上仍然可以降低误差 (左二有一定降低，左三与左四不太明显)，但是收敛性速度变慢了 (观察左二，左三，左四的稳定点出现的晚于左一)。]]
#### 渐进算法背后机理探究
##### 假说一，梯度消失导致训练精度低❎
为什么训练后期会进入一个平台期？如果统计此时的梯度分布情况，绘制其KDE ( kernel density estimation ) 曲线，那么分布应该是集中于0附近的，即梯度消失现象。
为什么会出现梯度消失呢，因为主干成分和次要成分之间的梯度相互抵消。
这个可以简单的做一个实验来证明。
##### 验证一，梯度分布
我统计了几个训练帧的梯度分布情况，绘制了其KDE曲线 (Gaussian kernel)。
但是这样是不能说明问题的，因为采样密度不够多，不能表现出想要的结果，有些可能正好处于尖端处，有些店可能是处于平台期，对比性不强。所以采用了，先选定几个大的训练帧，然后在训练帧的附近采样多个求平均。具体来说，我选取了几个点[渐进终止/2，渐进终止，渐进终止*2，渐进终止*4]，然后在这四个点的[-50,50]领域内，随机选取了10个点，然后记录。
因为神经网络有四层，所以分别把每一层的weights和bias的梯度取出来，绘制其概率分布，由于训练到后期时大部分梯度值都接近0，且近似满足正态分布，所以比较其面积即可，但是发现使用渐进方法的模型，梯度的分布函数比标准模型更接近0。从梯度消失的层面解释，似乎是行不通的。
![[Pasted image 20231028112001.png|不同模型的第一层梯度分布，当前迭代步为1600步。]]
![[Pasted image 20231028112025.png|不同模型的第二层梯度分布，当前迭代步为1600步。]]
![[Pasted image 20231028112043.png|不同模型的第三层梯度分布，当前迭代步为1600步。]]
![[Pasted image 20231028112100.png|不同模型的第四层梯度分布，当前迭代步为1600步。]]
为了更进一步的展示到底哪一种模型梯度消失现象更加严重，我统计了梯度中小于1e-4的数量，得出如下表格。表格中数对(m,n)代表小于1e-4的weights和bias数量。

| 模型         | 第一层   | 第二层    | 第三层    | 第四层 |
| ------------ | -------- | --------- | --------- | ------ |
| original     | 192, 103 | 1790, 96  | 2166, 94  | 82, 4  |
| linear       | 151, 68  | 1948, 111 | 2649, 127 | 85, 9  |
| concave_up   | 178, 81  | 2052, 124 | 2729, 151 | 100 ,9 |
| concave_down | 198,99   | 1906, 107 | 2655, 122 | 110, 9 | 
结果显示，采用了渐近方法后确实梯度消失现象更加严重了。但是，我们确实看到其精准度提高了。😰

我猜测出现这种情况的原因还是在记录点的选取上，即使在备选点的周围随机采样，仍然避免不了采样到一些尖端点。但是如果以L2曲线下降的具体数值来确定，记录当L2曲线的值等于某个特定值时的梯度，也许能消除这种影响。
记录了当相对l2误差达到[0.8,0.4,0.2]时刻的梯度，但是观察到仍然满足标准模型的梯度分布更加均匀，而采用了渐进算法的模型梯度分布都较为集中，且0附近的概率密度较大。
![[Pasted image 20231028211318.png|Relative l2误差为0.2时，第二层梯度分布。]]
![[Pasted image 20231028211247.png|Relative l2误差为0.2时，第二层梯度分布。]]
![[Pasted image 20231028211220.png|Relative l2误差为0.2，第三层梯度分布。]]
![[Pasted image 20231028211156.png|Relative l2误差为0.2，第四层梯度分布。]]
上面只展示了误差为0.2时刻的梯度分布，其他时刻也有相同的规律，即分布的集中程度上，concave_up>linear>concave_down>original，值得注意的是，训练完毕的精度上，也满足，concave_up>linear>concave_down>original。
这似乎有些违背直觉，不应该是梯度分布越集中，则梯度消失越明显，模型的训练越容易失败吗？或者我们可以理解为，梯度分布的越集中，说明模型总是以小步长进行迭代，从而避免了因为学习率过大而引起的训练失败。
##### 假说二，梯度分布分散导致训练精度低✅
前面我们通过记录梯度分布，发现了模型的梯度分布与其准确率存在一定相关性，且表现为，梯度分布越集中，精度越高。
所以会不会是在这里，主要影响的因素不是梯度消失，而是梯度分散呢？
梯度分散会导致训练不稳定，导致模型训练变慢，出现震荡。此时通过微调学习率可以改善情况，但是在整个训练环节中，我们一直没有调过学习率，所以会不会是学习率太大了，导致后期训练时候的步长太大。
如果要验证这个假说，可以缩小调整学习率，使标准模型的精度逼近渐进模型的精度，然后对比其梯度，若此时调整后的模型的梯度梯度分布也接近渐进模型 (表现得较为集中而不是分散)，就可以说明，是梯度的分布散乱导致的训练失败 (精度低，难收敛)。
##### 验证二，与余弦退火学习率模型梯度比较
前面我们假设了造成训练结果不好的原因是，梯度分布太分散了，加之学习率过大，每次迭代的步长太大，从而导致模型训练时候产生震荡，难以向着正确的方向收敛。
在这里我们做了一个验证实验，由于标准模型在固定学习率为1e-3的情况下训练20k步已经基本收敛，继续迭代并没有太显著的提升。这里采用了余弦退火学习率，具体的学习率变化过程如下图所示：
![[Pasted image 20231030212954.png|总共迭代10100步，初始学习率1e-3，采用余弦函数逐渐降低学习率，图中没有体现出退火的回升阶段，是因为周期设置的刚好是10100步时候回复至初值。]]
虽然这里的实现方式是余弦退火，但是回升的间隔设置的比较大，或许叫动态学习率手段更合适，毕竟这段学习率并没有回升阶段，只有不断的下降，标准模型可以在100k步时将误差降低到0.06。
![[Pasted image 20231030214717.png|在标准模型施加动态学习率后，与标准模型以及渐进算法模型的L2误差对比。图中可以看到施加了动态学习率后的标准模型在训练足够多的批次后，误差得到了进一步降低。]]
绘制这四种模型在相对误差接近0.2附近时的梯度分布。
![[Pasted image 20231030214954.png|第一层weight梯度分布。红色曲线代表采用了退火学习率 (动态学习率)，其梯度分布比标准模型和linear模型更集中。]]
![[Pasted image 20231030215347.png|第二层weight梯度分布。红色曲线代表采用了退火学习率 (动态学习率)，其梯度分布比标准模型和linear模型更集中。]]![[Pasted image 20231030215429.png|第三层weight梯度分布。红色曲线代表采用了退火学习率 (动态学习率)，其梯度分布比标准模型和linear模型更集中。]]
![[Pasted image 20231030215501.png|第四层weight梯度分布。红色曲线 (退火学习率)的梯度分布最为集中。]]
以上例子中，我们发现，就梯度的分布而言 (除了最后一层)，都满足，concave_up渐进模型 > 施加了退火学习率的标准模型>linear渐进模型>标准模型，而恰好这四者之间的精度 (误差) 也满足如此关系。
总结来说，近乎满足一个规律，梯度分布的越集中的，精度反而越高，误差越小。这说明，影响该方程的主要因素，并不是所谓的梯度消失现象，反而是梯度分布过于松散，而学习率得不到合理的调整，导致的迭代步长过大而引起的震荡。
注意，至此，我们所讨论的一切梯度都只是PDE的梯度，并不包含BC、IC等其他损失函数的梯度，因为我们的方法自始至终只调整了PDE内部的关系。
#### 机理探究的总结✅
上一小节给出了一个看似十分荒唐的答案，就如同对着答案出题。
但是这一发现好像蛮有意思，联合动态权重的思想，总结起来就是，PDE内部各项的训练因梯度过于分散而导致了训练缓慢，PDE与BC、IC等不同损失函数之间由于梯度相互干扰而导致训练缓慢。对于PINN损失函数 $\mathcal{L} = \mathcal{L}_{r}+ \mathcal{L}_{b} +\mathcal{L}_0$，不同损失函数之间$\mathcal{L}_r$与$\mathcal{L}_b$ 的梯度差距是量级之间的，所以主要体现为竞争关系，需要考虑梯度消失，给小梯度的项更大的权重；正相反，在单一损失函数中，如果该损失函数是一个多项式，那么多项式各个项之间的梯度分布应该是比较接近的，此时影响训练结果的是梯度分散导致的训练震荡。
简而言之，各损失函数之间的的梯度竞争关系是，小梯度的损失函数难以被充分训练，因此需要动态权重；单一损失函数内部，各个项之间竞争，梯度分布过于分散，导致震荡。
![[Pasted image 20231031092752.png|不同损失函数之间的梯度关系 (标准模型，相对误差为0.2时，第3层权重的梯度分布)。某些过于集中，在训练时修改的步长太小。所以影响总体训练的是梯度消失现象，因为梯度分布过于集中的损失函数没有被充分训练。]]

😰到这里为止，还是有bug的，即这里测试的时候，渐进比例只有0.2，也就是说，只有前20%的训练是受到渐进影响的，后面的80%的训练是正常训练，而我们测得的梯度是处于后80%正常训练范围的，为什么在训练早期施加了渐进，对中后期的训练起到了作用？
对此，我只能给出一种猜测，即早期通过削弱次要项的方法，使得PDE的主要特征被更好的学习到了，从而规避掉了部分局部最优。也就是说，前期的训练更像是在做一种预训练。可是，描述PDE的特征被学习的程度本身就是一个更加抽象的问题了，至此，我想不出什么好的方法去验证或者探究。

💡我的想法是，作为一种工程的算法，不必考虑科学性。虽然这和调参一样，本来就是拍脑袋想出来的，但是只要有充分的例子证明其具备优势，虽然具体机理没有研究清楚，可以作为 future works 进行讨论。

## 余弦退火学习率 cosine annealing warm 
实际上学习率应该是要变的，固定学习率只是为了方便比较。但是我观察用到退火算法的论文并不多，因为它也比较难调，一般多见于做应用的文章，理论研究的文章一般不会用退火。
![[Pasted image 20231025222227.png|余弦退火学习率示意图]]
退火学习率后仍然可以保证渐进方法的优越性，但是退火需要额外调参数。
退火的主要好处是，Loss曲线会平滑许多，波动整体会变少，但是也会出现一些明显的形如delta函数的抖动。
![[Pasted image 20231031090707.png|使用退火与不使用退火的对比。可以看到退火后的精度会高一些，并且平滑了许多。]]
## 动态权重
没必要了，复现效果并不好。
如有必要，可以将其视为与退火算法等一类的，与本算法本质上不冲突的算法。因为是从不同的视角考虑问题，退火考虑的是学习率，动态权重考虑PDE与BC，本算法考虑主次关系，这几个因素是相互独立的，所以算法可以联合使用，并且联合后的效果优于其中单独一种。
既然退火复现出来了，那干脆调退火就行了，到时候可以举退火的例子来证明上面的说法，从而规避了讨论动态权重算法。