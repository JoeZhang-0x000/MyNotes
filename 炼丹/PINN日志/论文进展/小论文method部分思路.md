为了使得method部分上手写的时候条理清晰，我将几种的能想到的叙述方式总结如下。以便于后续斟酌选取其中一种来作为实际的行文思路。
# 思路一，从问题出发
从问题的本身开始讲，通过前人的研究以及我们的假设，自然的推出我们的方法。
主要按照如下的小标题来叙述：
1. Problem setup
2. Method motivation
3. Progressive Emphasis Training Strategy
4. Assessing Component Impact
如下是每一个模块的详细思路
## Problem setup
问题建立。
本小节主要是作为背景知识，为读者介绍PINN的一般形式，优化这样一个式子:
$$
\mathcal L = \mathcal{L}_r+\mathcal{L}_b+\mathcal{L}_0
$$
PINN的核心目的就是要找到合适的神经网络参数$\theta$ 使得损失函数$\mathcal{L}$最小化。此时我们认为该套参数$\theta$控制的神经网络$u(\theta)$ 就是原PDE的解。这个在数学上已经被证明了是可行的，但是实际中往往训练的不尽人意，并且PINN不同于conventional method，不能通过直接的加密网格获得更高精度解，所以几乎所有PINN相关工作的努力就是找到合适的算法让$\mathcal{L}$收敛的更快并且更接近0。
## Method Motivation
方法动机。
目前的state of art的PINN仍然主要使用fully connected neural network (FNN)，而FNN近期被证明了具有spectral bias (频谱偏差)，即对高频的表征能力存在缺陷。与此同时，也有其他学者通过NTK (神经网络正切核) 的研究 (简而言之，神经网络的收敛速度可以由NTK的eigenvalue大小进行预测，eigenvalue越大则对应的反向传播的梯度越大，收敛的越快，但是作者发现PINN的NTK存在病态性质，即eigenvalue很快衰减到0，并且PDE residual的eigenvalue远大于Boundary condition的，这说明这两项$\mathcal{L}_{r}\text{ and } \mathcal{L}_b$之间存在收敛的不平衡，会产生竞争关系，进而导致训练结果不好)，发现PINN的损失函数中各项 (PDE残差和边界条件等) 之间存在竞争关系，即收敛的速度不统一，导致了有些项的训练被弱化了，进而导致训练失败。
基于以上两个PINN训练失败的分析，我们认为
1. PDE的表达式中各个项对解的数值影响并不是统一的，这里可以做一个小实验，将其中某些项加一点扰动然后看结果的偏离程度，从而就可以区分出对结果影响大的主干项和对结果影响小的次要项；
2. 除了PDE残差和边界条件等使用损失函数显式表征的项之外，在PDE中，还有一些没有显式表达出来的项的损失，例如方程中的某些项距离其理论数值的距离。这些隐式的损失之间也会存在相互的竞争关系，例如高频和低频项之间，高阶导数和低阶导数项之间。我们认为，这些项之间的训练速度不统一影响了PDE的训练；
3. 想要平衡隐式存在的这些项之间的训练关系并不容易，但是我们可以有选择性的忽略某些次要项的训练，加强主要项的训练，从而在更大的程度上保证我们的结果更加准确。同时，在一定程度上，由于削弱了训练时次要项对主干项的影响，所以主干项的训练速度加快了，体现在PINN的训练中就是收敛的速度更快。此外，由于我们更加强调主干项的训练，这一部分收敛的会比次要项更好，所以体现在PINN的结果中我们的训练结果精度也变高了。
## Progressive Emphasis Training
渐进式重点训练策略。
这里主要阐述我们的具体训练策略。首先需要基于对方程本身的一些先验知识，判断哪些项对结果的影响大，哪些项对结果的影响较小。然后将训练过程分为两个阶段：
1. 第一个阶段内主要训练主干项，通过给次要项前面增加一个缩小因子 (小于1) 而减少神经网络对次要项的关注程度，或者给主要项前面加一个扩大因子 (大于1)；
2. 第二个训练阶段将所有的放缩因子恢复至1，神经网络平等的训练这两项；

## Assessing Component Impact
方程中成分的评估。
直到这里我们都还没有谈及如何确定方程中的主干项和次要项，我们只是从定义上来说，通过给一个微小扰动，看结果的变化程度，但是这个操作还是需要很大的计算量的，所以在本文的实验中我们提出了一种比较可靠的方法来粗略的定量估计方程中各项之间的重要程度。
在这里我们通过最大信息系数 (MIC)对方程中各个项与方程解的相关程度来定量衡量其对结果的影响程度，MIC值越大，说明这一项与结果的相关性越大，即这一项的变化会引起结果的较大变化，这一项就是比较重要的项。
在具体的使用过程中，我们不必获取PDE确切的解，可以通过其coarse grained解进行判断，或者可以用相似的方程进行估计。但是在本文中，为了简便起见，同时为了更好的呈现效果，我们采用了PDE的参考解进行计算MIC，我们下一节Experiments中所涉及的各个PDE的各个项的MIC值将会通过一个表格呈现给读者。

# 思路二，从示例出发
从一个示例出发，
本思路的主要区别在于第二小节如何引入PDE表达式中隐式存在的竞争关系。上一种思路是根据前人的研究内容，进一步外推，假设了PDE表达式内部也有隐式的竞争关系，不仅仅是显示表现出来的PDE residual同Bounary Condition之间的竞争；思路二则是，通过一个具体的例子去切入。==但是这个例子我目前没有想好==😿。我最近几天也在做一个小实验去验证，例如，我们考虑一个bugers方程:
$$
u_{t} + uu_{x} - \nu u_{xx} = 0
$$
我们将方程拆成两部分，
$$
\begin{aligned}
u_{t}+uu_{x} = \gamma \cr
\gamma-\nu u_{xx} =0
\end{aligned}
$$
这样岂不是将要原来要优化的,
$$
\mathcal L = \mathcal{L}_{r}+ \mathcal{L}_b+\mathcal{L}_0
$$
变成了，
$$
\mathcal L = \mathcal{L}_{r_1}+\mathcal{L}_{r_2}+ \mathcal{L}_b+\mathcal{L}_0.
$$
如此一来，我们只要仿照原始论文中研究$\mathcal{L}_{r_1}$以及$\mathcal{L}_{r_2}$的NTK特征值即可，如果发现这两项也存在极大的特征值差异，就可以说明受到了竞争关系。
目前的主要阻碍在于怎么求NTK特征值，数学上不会推导。🥲
此外，如果采用这种拆分的方式，implicit的竞争关系变成了explicit，那么我们似乎可以采用动态权重去进行了。为什么我们不直接拆分，然后采用动态权重呢，因为这样带来了额外的计算开销，动态权重当然会是一种可能的解决方案，但是本文我们提出了一种替代的手段，其实现的计算开销更小，且，与动态权重并不冲突，可以结合使用。

这里列出的只是小标题的另一种备选写法。
其小标题安排如下：
1. Introduction to Physics-informed Neural Networks
2. Implicit Competition Among Terms in PDE
3. Progressive Emphasis Training Strategy
4. Evaluating Term Impact with the Maximum Information Coefficient