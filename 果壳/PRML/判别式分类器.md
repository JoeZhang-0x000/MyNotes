# 判别式分类
前面我们介绍了生成式分类方法，其主要工作是估计出样本处于每一个类别的概率密度函数，然后根据输入处于哪一个类别的概率更大来进行判决。
在很多时候，准确的估计概率密度函数并不是一件容易的事情，实际上模型识别的目的主要是在特征空间中设法找到两个类别(或者多个类别)之间的分类面，估计概率密度函数并不是我们的主要目的。
本节主要讨论判别式分类方法，其主要目的是直接找到分类的边界，而不是去估计样本的概率密度函数。
打个比方，例如对语言进行分类，生成式分类就是要先学会每种语言，然后再去看他更符合哪一种语言的特性；而判别式分类则直接通过两种语言的差异，例如字符，音调，拼写等，这显然更加容易。
# 生成式 VS 判别式
- 生成式模型的学习收敛速度更快，样本数量越多，越容易收敛于接近真实的模型；
- 生成模型可以应对存在隐变量的情况；

while
- 判别模型需要的计算量小；
- 实践中多数情况下判别式模型的好于生成式

# 线性判别函数
最简单的判别式模型就是线性模型了，广义的来说，可以通过一些投影变换将非线性的空间转化为线性空间，因此线性判别的应用十分广泛。
![[Pasted image 20230927150320.png]]

## 线性判别函数
首先从最简单的形式开始研究，即二分类问题，类别为$\{\omega_1,\omega_2\}$。
输入$d$个属性描述的示例$\boldsymbol x=(x_1,x_2,\cdots,x_d)$，则可以设一个线性函数：
$$
\begin{aligned}
d(x) &=w_{1}x_1+w_{2}x_2+\cdots+w_dx_d+w_{d+1} \cr
&=W^{T}\boldsymbol {x} +W_{d+1}
\end{aligned}
$$
其中，$\boldsymbol{x}=\{x_1,x_2,\cdots,x_{d}\},W=\{w_1,w_2,\cdots,w_d \}^T$$\boldsymbol{x} \text{被称为模式向量, } W \text{被称为权向量}$。
也有一些教材会将$\boldsymbol{x}=\{x_1,x_2,\cdots,x_{d},1\},W=\{w_1,w_2,\cdots,w_d,w_{d+1} \}^T$，$\boldsymbol{x} \text{被称为增广模式向量, } W \text{被称为增广权向量}$，不同的叫法罢了。

## 二分类问题
$$

d(x) = W^Tx= \left \{
\begin{aligned}
&>0 \quad \text{if } x\in\omega_1 \cr
&<0 \quad \text{if } x\in \omega_2  \cr
\end{aligned}
\right .
$$
若等于0，则随便取哪一类都可以，即为分类边界。
## 多分类问题
处理二分类问题时候可以简单规定判别函数>0取1类，判别函数<0取2类，但是面对多分类问题时候，就不能简单的通过大于零小于零来判决了。
一般的，有以下几种处理的trick。
### $\omega_{i}/\tilde \omega_{i}$两分法
最直觉的一种分法，即对于每一个类别$\omega_i$ 都去做一个判决函数$d_i(x)$.
$$
d_i(x)=
\left \{
\begin{aligned}
&>0 \quad \text{if }x\in\omega_i \cr
&<0 \quad \text{if }x \notin \omega_i \cr
\end{aligned}
\right .
$$
这被称为$\omega_{i}/\tilde \omega_{i}$两分法。把M类多分类问题，转化成为M个二分类问题，因此一共有M个判别函数。
在两分法的情况下，
- $x\in\omega_i$ if and only if
$$
\begin{aligned}
& d_i(x) >0 \cr
& and \cr
& \forall j\ne i  \quad d_j(x) <0
\end{aligned}
$$
- 分类失败，存在不确定区域(IR)
	- $\forall i \quad d_i(x)<0$,
	- or，$\exists i\ne j \quad d_i(x)>0,d_j(x)>0$ .

例如，
![[Pasted image 20230927162508.png]]
### $\omega_{i}/\omega_j$ 两分法
这种方法的思想是，对于任意的$i \ne j$，都去构造一共判别函数$d_{ij}(x)$，将i和j区分开。
$$
\begin{aligned}
& x \in \omega_i \quad & \text{if and only if} \quad \forall i\ne j \quad d_{ij} >0 \cr
& IR \quad & \text{if and only if } \forall i \quad \exists j\ne i \quad d_{ij}<0
\end{aligned}
$$
采用此种二分法的计算量比较大，对于M类问题，需要构建的判别函数有$C^2_M$个。
![[Pasted image 20230927162441.png]]
### $\omega_{i}/\omega_j$ 两分法的特殊情况
前面提到$\omega_{i}/\omega_j$ 两分法的复杂度较高，因为需要划分$C^2_M$个二分类问题，但是对于灭有IR区域的特殊情况，可以进一步简化。
$$
\begin{aligned}
& x \in \omega_i \quad \text{if and only if} \cr & \forall j \ne i \quad d_{ij}(x) = d_i(x)-d_j(x)>0
\end{aligned}
$$
也可以写作：
$$
\begin{aligned}
& x\in\omega_i \quad \text{if and only if} \quad d_i(x) = \max_j{d_j(x)}, \cr 
& \quad j \in\{1,2\cdots,M\}.
\end{aligned}
$$若可以如此简化，则最终只需要M个判别函数即可。这相当于把M类问题转化成了M-1个二分类问题。
![[Pasted image 20230927163454.png]]
# 广义线性判别函数
人类喜欢线性的东西，因为其只管，简单，一个好的算法也应该如此。但是狭义上的线性模型的表征能力有限。能不能通过一定的手段，增强其对非线性划分的能力？
假设一个分类问题在模式空间x中线性不可分，那我们可以尝试构造一种非线性的映射关系，使得其在$x^\star$中线性可分。
比较抽象，举一个例子吧，
例如我们要处理如下的分类问题：
![[Pasted image 20230927164537.png]]
要区分$\omega_{1},\omega_2$ 这个问题在一维的不可线性分，在这里我们做一个变换，令$\{x\}->\{x,x^2\}$，则在二维平面上，实现了线性分割。
![[Pasted image 20230927165410.png]]
# 分段线性函数
虽然上一节给出了一种广义线性模型，通过构造非线性的项，将低维线性不可分问题转化高维空间中的线性可分问题。
但是随着维数的增大，计算量会指数级别的增长，所以我们仍然希望在尽量不要增加问题的维度。
在这里我们引入分段线性函数，是一种折中的手段。
如下图所示，对于这样一个多分类问题，我们可以如下的分段线性模型进行判别。
![[Pasted image 20230927170410.png]]
具体来说，如何实现呢？
一种常用的算法是最小距离分类，他的思想就是“找距离最近的类”，我们先求出每一个类别的中心向量位置，然后对于给定的$\boldsymbol{x}$ 求它到每一个类别中心的距离，然后将其归结到距离最近的类别中。
这里的距离度量方式可以有很多种，最常见的是欧氏距离，此时分类的边界就是两个相邻类的中心的垂直平分线。
采用不同的距离计算方式，其最终的分类边界会有区别，需要根据实际问题具体选择。
对于上图中的$m_3$类，为了更好的分类效果，我们对其进行了切分。
# Fisher 线性判别
人类的认知范围是有限的，因此更加偏爱低维的事物，例如对于一个三维以上的系统，就很不直观了。我们希望能够将高维问题转化为低维问题来做，其一是因为低维更直观，其二是因为低维的计算量更小，更容易实现。
有没有一种方法可以把d维空间中的样本投影到一条直线上，然后在一维空间上进行划分？
Fisher线性判别的目标就是找到这样一个投影线，并且是最优的投影线。
![[Pasted image 20230927174612.png]]
何为最优？我们希望，投影后各类别的样本与样本之间越分散越好，同时样本内部越集中越好，这样方便我们进行划分。
以二分类为例，
对于$d\text{维空间，各样本为}\boldsymbol{x}=\{x_1,x_2,\cdots,x_d\}$可以划分到两个类别$\{ \omega_1,\omega_2\}$。
则样本类内的均值为：
$$
m_i = \frac{1}{n}\sum x_i
$$
样本类内离散度矩阵：
$$
S_i = \frac{1}{n}\sum (\boldsymbol{x}-m_i)(\boldsymbol{x}-m_i)^t
$$
两个样本类之间的均值反映了两个类别的分布的距离的远近，例如$(m_1-m_2)^2$越大，表示两个样本类的中心点的距离越远，则样本分布的越远。
样本类内的方差又代表了样本内部的聚集程度。
因此定义
$$
J_F(W) = \frac{(m_1-m_2)^2}{S_1^2+S_2^2}
$$
Fisher准则的目标就是求$J_F(W)$取最大值的投影方向$W$，即
$$
W^* = \arg \max_W J_F(W)
$$
也就是让分子尽量大，分母尽量小。

将$W$带入，然后采用lagrange乘数法求极值点即可。
数学推导略。
可参考：
https://seanwangjs.github.io/2017/11/28/fisher-linear-discriminant-analysis.html


